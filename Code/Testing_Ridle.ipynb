{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85fdaab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras import backend as K\n",
    "import argparse\n",
    "import os\n",
    "from sentence_embeddings import Sentence_Embedder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142db268",
   "metadata": {},
   "source": [
    "# Ridle (with and without Language Model)\n",
    "In this notebook, the performance of \"Ridle\" is tested on different datasets. The neural network for further testing wasn't changed in any way, except giving it different inputs. Firstly the model is tested by itself to see how it performs in the normal case of just the representation from the RBM as inputs. Then we add our sentence embedding vectors to the representation vector and concatenate them to receive a new bigger vector with the size of input (50) and sentence embedding (384). Finally the whole program is also tested with only the sentence embeddings as inputs, to see how well the program can classify just by getting information from the summary embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e75c7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#jupyter notebook --NotebookApp.iopub_data_rate_limit=1.0e10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e9aaed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GELU Activation function\n",
    "def gelu(x):\n",
    "    return 0.5 * x * (1 + K.tanh(x * 0.7978845608 * (1 + 0.044715 * x * x)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f309dbb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_ridle = []\n",
    "parser = argparse.ArgumentParser(\n",
    "    description='Instance Type Prediction using Ridle',\n",
    ")\n",
    "parser.add_argument('--dataset', nargs='?', default='ChemicalCompounds_Dbpedia', type=str)\n",
    "#parser.add_argument('--dataset', nargs='?', default='DBp_2016-04', type=str)\n",
    "parser, unknown = parser.parse_known_args()\n",
    "\n",
    "# Load Representations\n",
    "print('Reading Data...')\n",
    "df = pd.read_csv('./dataset/{}/embedding.csv'.format(parser.dataset))\n",
    "\n",
    "\n",
    "# Load mapping\n",
    "if 'dbp' in parser.dataset.lower():\n",
    "    mapping = pd.read_json('./dataset/dbp_type_mapping.json')\n",
    "elif 'wd' in parser.dataset.lower() or 'wikidata' in parser.dataset.lower():\n",
    "    mapping = pd.read_json('./dataset/wd_mapping_type.json')\n",
    "else:\n",
    "    mapping = pd.read_json('./dataset/{}/type_mapping.json'.format(parser.dataset))\n",
    "\n",
    "\n",
    "# merge them\n",
    "print('Processing Data...')\n",
    "r = pd.merge(df, mapping, on='S')\n",
    "\n",
    "K_FOLD = 10\n",
    "mlb = MultiLabelBinarizer()\n",
    "fold_no = 1\n",
    "loss_per_fold, f1_macro, f1_micro, f1_weighted = [], [], [], []\n",
    "kfold = KFold(n_splits=K_FOLD, shuffle=True, random_state=42)\n",
    "targets = mlb.fit_transform(r['Class'])\n",
    "inputs = r.drop(['S', 'Class'], axis=1).values\n",
    "for train, test in kfold.split(inputs, targets):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(inputs[train].shape[1], input_dim=inputs[train].shape[1]))\n",
    "    model.add(Activation(gelu, name='Gelu'))\n",
    "    model.add(Dense(targets[train].shape[1], activation='sigmoid'))\n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    print('Training...')\n",
    "    history = model.fit(inputs[train], targets[train], batch_size=64, validation_data=(inputs[test], targets[test]), epochs=100)\n",
    "    y_pred = model.predict(inputs[test])\n",
    "    y_pred[y_pred>=0.5]=1\n",
    "    y_pred[y_pred<0.5]=0\n",
    "    \n",
    "        \n",
    "    \n",
    "    # Generate F1 scores\n",
    "    scores = model.evaluate(inputs[test], targets[test], verbose=0)\n",
    "    f1_macro.append(f1_score(targets[test], y_pred, average='macro', zero_division=1))\n",
    "    f1_micro.append(f1_score(targets[test], y_pred, average='micro', zero_division=1))\n",
    "    f1_weighted.append(f1_score(targets[test], y_pred, average='weighted', zero_division=1))\n",
    "\n",
    "    for m in range(len(test)):\n",
    "        comp = targets[test][m] ==  y_pred[m]\n",
    "        if not(comp.all()):\n",
    "            wrong_ridle.append((test[m], y_pred[m], targets[test][m]))    \n",
    "            #wrong_ridle.append(test[m])\n",
    "    print('Score for fold', fold_no, ':', model.metrics_names[0], 'of', scores[0], ';', 'F1-Macro:', f1_macro[-1], 'F1-Micro:', f1_micro[-1])\n",
    "    loss_per_fold.append(scores[0])\n",
    "\n",
    "    fold_no += 1\n",
    "    t1 = targets\n",
    "# Provide average scores\n",
    "print('------------------------------------------------------------------------')\n",
    "print('Score per fold')\n",
    "for i in range(0, len(loss_per_fold)):\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print('> Fold', i+1, ' - Loss:', loss_per_fold[i], '- F1-Macro:', f1_macro[i], '%')\n",
    "print('------------------------------------------------------------------------')\n",
    "print('Average scores for all folds:')\n",
    "print('> F1-Macro:', np.mean(f1_macro), '(+-', np.std(f1_macro), ')')\n",
    "print('> F1-Micro:', np.mean(f1_micro), '(+-', np.std(f1_micro), ')')\n",
    "print('> Loss:', np.mean(loss_per_fold))\n",
    "print('------------------------------------------------------------------------')\n",
    "\n",
    "# Save results to file\n",
    "result = {}\n",
    "f1_macro = np.array(f1_macro)\n",
    "f1_micro = np.array(f1_micro)\n",
    "f1_weighted = np.array(f1_weighted)\n",
    "result['F1-Macro'] = np.mean(f1_macro)\n",
    "result['F1-Macro_std'] = np.std(f1_macro)\n",
    "result['F1-Micro'] = np.mean(f1_micro)\n",
    "result['F1-Micro_std'] = np.std(f1_micro)\n",
    "result['F1-Weighted'] = np.mean(f1_weighted)\n",
    "result['F1-Weighted_std'] = np.std(f1_weighted)\n",
    "result['Dataset'] = parser.dataset\n",
    "result['method'] = 'Ridle'\n",
    "df_result = pd.DataFrame([result])\n",
    "print(df_result)\n",
    "\n",
    "if os.path.isfile('./evaluation_instance_type.csv'):\n",
    "    df_result.to_csv('./evaluation_instance_type.csv', mode='a', header=False, index=False)\n",
    "else:\n",
    "    df_result.to_csv('./evaluation_instance_type.csv', index=False)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f75a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_id_ridle = []\n",
    "for element in wrong_ridle:\n",
    "    wrong_id_ridle.append(element[0])\n",
    "    wrong_id_ridle.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ff95a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_ridle = []\n",
    "for element in wrong_ridle:\n",
    "    pred_ridle.append(element[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c9b93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets_ridle = []\n",
    "for element in wrong_ridle:\n",
    "    targets_ridle.append(element[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f06732",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_pickle(\"./Misclassification.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5c89c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.at['{}'.format(parser.dataset), \"Ridle misclassified\"] = len(wrong_ridle)\n",
    "df1.at['{}'.format(parser.dataset), \"Prediction Ridle\"] = pred_ridle\n",
    "df1.at['{}'.format(parser.dataset), \"Target Ridle\"] = targets_ridle\n",
    "df1.at['{}'.format(parser.dataset), \"ID list Ridle\"] = wrong_id_ridle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1275ff3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.to_pickle(\"./Misclassification.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
