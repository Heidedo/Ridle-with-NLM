{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f74b66b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error: 0.05555555555555555: 100%|██████████| 1000/1000 [00:01<00:00, 603.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual\n",
      "[[1 1 1 0 0 0]\n",
      " [1 0 1 0 0 0]\n",
      " [1 1 1 0 0 0]\n",
      " [0 0 1 1 1 0]\n",
      " [0 0 1 1 0 0]\n",
      " [0 0 1 1 1 0]]\n",
      "Predicted\n",
      "[[1.   0.66 1.   0.   0.   0.  ]\n",
      " [1.   0.77 1.   0.   0.   0.  ]\n",
      " [1.   0.73 1.   0.   0.   0.  ]\n",
      " [0.   0.   1.   1.   0.64 0.  ]\n",
      " [0.   0.   1.   1.   0.1  0.  ]\n",
      " [0.   0.   1.   1.   0.57 0.  ]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from tqdm import trange\n",
    "import pickle\n",
    "\n",
    "class RBM():\n",
    "    \"\"\"Bernoulli Restricted Boltzmann Machine (RBM)\n",
    "    Parameters:\n",
    "    -----------\n",
    "    n_hidden: int:\n",
    "        The number of processing nodes (neurons) in the hidden layer. \n",
    "    learning_rate: float\n",
    "        The step length that will be used when updating the weights.\n",
    "    batch_size: int\n",
    "        The size of the mini-batch used to calculate each weight update.\n",
    "    n_iterations: float\n",
    "        The number of training iterations the algorithm will tune the weights for.\n",
    "    Reference:\n",
    "        A Practical Guide to Training Restricted Boltzmann Machines \n",
    "        URL: https://www.cs.toronto.edu/~hinton/absps/guideTR.pdf\n",
    "    \"\"\"\n",
    "    def sigmoid(self, x):\n",
    "        return 1.0 / (1 + np.exp(-x))\n",
    "    def batch_iterator(self, iterable, batch_size=1):\n",
    "        #l = len(iterable)\n",
    "        l = iterable.shape[0]\n",
    "        for ndx in range(0, l, batch_size):\n",
    "            yield iterable[ndx:min(ndx + batch_size, l)]\n",
    "\n",
    "\n",
    "    def __init__(self, n_hidden=128, learning_rate=0.1, batch_size=10, n_iterations=100):\n",
    "        self.n_iterations = n_iterations\n",
    "        self.batch_size = batch_size\n",
    "        self.lr = learning_rate\n",
    "        self.n_hidden = n_hidden\n",
    "        #self.progressbar = progressbar.ProgressBar()\n",
    "\n",
    "    def _initialize_weights(self, X):\n",
    "        n_visible = X.shape[1]\n",
    "        self.W = np.random.normal(scale=0.1, size=(n_visible, self.n_hidden))\n",
    "        self.v0 = np.zeros(n_visible)       # Bias visible\n",
    "        self.h0 = np.zeros(self.n_hidden)   # Bias hidden\n",
    "        self.grads_first_moment_W = np.zeros((n_visible, self.n_hidden))\n",
    "        self.grads_second_moment_W = np.zeros((n_visible, self.n_hidden))\n",
    "        self.grads_first_moment_v = np.zeros(n_visible)\n",
    "        self.grads_second_moment_v = np.zeros(n_visible)\n",
    "        self.grads_first_moment_h = np.zeros(self.n_hidden)\n",
    "        self.grads_second_moment_h = np.zeros(self.n_hidden)\n",
    "        self.beta1 = 0.9\n",
    "        self.beta2 = 0.999\n",
    "        self.epsilon = 1e-8\n",
    "\n",
    "        '''\n",
    "        self.beta1 = 0.1\n",
    "        self.beta2 = 0.4\n",
    "        self.epsilon = 1e-1\n",
    "        '''\n",
    "    def fit(self, X, y=None):\n",
    "        '''Contrastive Divergence training procedure'''\n",
    "\n",
    "        self._initialize_weights(X)\n",
    "\n",
    "        self.training_errors = []\n",
    "        self.training_reconstructions = []\n",
    "        #for _ in self.progressbar(range(self.n_iterations)):\n",
    "        t = trange(self.n_iterations, desc='Error: ',leave=True)\n",
    "        for time in t:\n",
    "            time += 1\n",
    "            batch_errors = []\n",
    "            for batch in self.batch_iterator(X, batch_size=self.batch_size):\n",
    "                # Positive phase\n",
    "                positive_hidden = self.sigmoid(batch.dot(self.W) + self.h0)\n",
    "                hidden_states = self._sample(positive_hidden)\n",
    "                positive_associations = batch.T.dot(positive_hidden)\n",
    "\n",
    "                # Negative phase\n",
    "                negative_visible = self.sigmoid(hidden_states.dot(self.W.T) + self.v0)\n",
    "                negative_visible = self._sample(negative_visible)\n",
    "                negative_hidden = self.sigmoid(negative_visible.dot(self.W) + self.h0)\n",
    "                negative_associations = negative_visible.T.dot(negative_hidden)\n",
    "\n",
    "\n",
    "                self.grads_first_moment_W = self.beta1 * self.grads_first_moment_W + \\\n",
    "                              (1. - self.beta1) * (positive_associations - negative_associations)\n",
    "                self.grads_second_moment_W = self.beta2 * self.grads_second_moment_W + \\\n",
    "                              (1. - self.beta2) * (positive_associations - negative_associations)**2\n",
    "\n",
    "                grads_first_moment_unbiased = self.grads_first_moment_W / (1. - self.beta1**time)\n",
    "                grads_second_moment_unbiased = self.grads_second_moment_W / (1. - self.beta2**time)\n",
    "                \n",
    "                self.W += self.lr * grads_first_moment_unbiased /(np.sqrt(grads_second_moment_unbiased) + self.epsilon)\n",
    "                  \n",
    "                \n",
    "                self.grads_first_moment_h = self.beta1 * self.grads_first_moment_h + \\\n",
    "                              (1. - self.beta1) * (positive_hidden.sum(axis=0) - negative_hidden.sum(axis=0))\n",
    "                self.grads_second_moment_h = self.beta2 * self.grads_second_moment_h + \\\n",
    "                              (1. - self.beta2) * (positive_hidden.sum(axis=0) - negative_hidden.sum(axis=0))**2\n",
    "                \n",
    "                grads_first_moment_unbiased = self.grads_first_moment_h / (1. - self.beta1**time)\n",
    "                grads_second_moment_unbiased = self.grads_second_moment_h / (1. - self.beta2**time)\n",
    "                \n",
    "                self.h0 += self.lr * grads_first_moment_unbiased /(np.sqrt(grads_second_moment_unbiased) + self.epsilon)\n",
    "\n",
    "\n",
    "                self.grads_first_moment_v = self.beta1 * self.grads_first_moment_v + \\\n",
    "                              (1. - self.beta1) * (batch.sum(axis=0) - negative_visible.sum(axis=0))\n",
    "                self.grads_second_moment_v = self.beta2 * self.grads_second_moment_v + \\\n",
    "                              (1. - self.beta2) * (batch.sum(axis=0) - negative_visible.sum(axis=0))**2\n",
    "\n",
    "                grads_first_moment_unbiased = self.grads_first_moment_v / (1. - self.beta1**time)\n",
    "                grads_second_moment_unbiased = self.grads_second_moment_v / (1. - self.beta2**time)\n",
    "\n",
    "                self.v0 += self.lr * grads_first_moment_unbiased /(np.sqrt(grads_second_moment_unbiased) + self.epsilon)\n",
    "\n",
    "                batch_errors.append(np.mean((batch - negative_visible) ** 2))\n",
    "\n",
    "            self.training_errors.append(np.mean(batch_errors))\n",
    "            t.set_description('Error: {e}'.format(e = self.training_errors[-1]))\n",
    "            t.refresh() # to show immediately the update\n",
    "            # Reconstruct a batch of images from the training set\n",
    "            idx = np.random.choice(range(X.shape[0]), self.batch_size)\n",
    "            self.training_reconstructions.append(self.reconstruct(X[idx]))\n",
    "\n",
    "    # Implemented by me, does the same as reconstruct. But some computations are not needd.\n",
    "    def predict(self, X):\n",
    "        # Positive phase\n",
    "        positive_hidden = self.sigmoid(X.dot(self.W) + self.h0)\n",
    "        hidden_states = self._sample(positive_hidden)\n",
    "        positive_associations = X.T.dot(positive_hidden)\n",
    "\n",
    "        # Negative phase\n",
    "        negative_visible = self.sigmoid(hidden_states.dot(self.W.T) + self.v0)\n",
    "        #negative_visible = self._sample(negative_visible)\n",
    "        negative_hidden = self.sigmoid(negative_visible.dot(self.W) + self.h0)\n",
    "        negative_associations = negative_visible.T.dot(negative_hidden)\n",
    "        \n",
    "        return negative_visible\n",
    "\n",
    "    def predict_count(self, X):\n",
    "        # Positive phase\n",
    "        positive_hidden = self.sigmoid(X.dot(self.W) + self.h0)\n",
    "        hidden_states = self._sample(positive_hidden)\n",
    "        negative_visible = self.sigmoid(hidden_states.dot(self.W.T) + self.v0)\n",
    "        \n",
    "        return negative_visible, hidden_states\n",
    "        \n",
    "    def _sample(self, X):\n",
    "        return X > np.random.random_sample(size=X.shape)\n",
    "\n",
    "    def reconstruct(self, X):\n",
    "        positive_hidden = self.sigmoid(X.dot(self.W) + self.h0)\n",
    "        hidden_states = self._sample(positive_hidden)\n",
    "        negative_visible = self.sigmoid(hidden_states.dot(self.W.T) + self.v0)\n",
    "        return negative_visible\n",
    "     \n",
    "    def compress(self, X):\n",
    "        positive_hidden = self.sigmoid(X.dot(self.W) + self.h0)\n",
    "        hidden_states = self._sample(positive_hidden)\n",
    "        return positive_hidden\n",
    "\n",
    "    def save(self, name):\n",
    "        \"\"\"save class as self.name.txt\"\"\"\n",
    "        with open(name, 'wb') as handle:\n",
    "            pickle.dump(self.__dict__, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    def load(self, file):\n",
    "        \"\"\"try load self.name.txt\"\"\"\n",
    "        with open(file, 'rb') as handle:\n",
    "            tmp_dict  = pickle.load(handle)\n",
    "        self.__dict__.update(tmp_dict) \n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    X = np.array([[1,1,1,0,0,0],[1,0,1,0,0,0],[1,1,1,0,0,0],[0,0,1,1,1,0], [0,0,1,1,0,0],[0,0,1,1,1,0]])\n",
    "    rbm = RBM(n_hidden=100, n_iterations=1000, batch_size=25, learning_rate=0.1)\n",
    "    rbm.fit(X)\n",
    "    print('Actual')\n",
    "    print(X)\n",
    "    print('Predicted')\n",
    "    print(np.around(rbm.predict(X), decimals=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
